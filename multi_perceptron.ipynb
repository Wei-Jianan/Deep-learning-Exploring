{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mxnet import nd, gluon\n",
    "from mxnet import autograd as ag\n",
    "import mxnet as mx\n",
    "import utils\n",
    "\n",
    "ctx = utils.try_gpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "784"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 256\n",
    "epochs = 100\n",
    "learning_rate =0.5\n",
    "train_data, test_data = utils.load_data_fashion_mnist(batch_size)\n",
    "28 * 28"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_inputs = 28 * 28\n",
    "num_hidden_1 = 256 \n",
    "\n",
    "num_outputs = 10\n",
    "weight_scale = 0.01\n",
    "\n",
    "W1 = nd.random_normal(scale=weight_scale, shape=(num_inputs,num_hidden_1),ctx=ctx)\n",
    "b1 = nd.random_normal(scale=weight_scale, shape=num_hidden_1, ctx=ctx)\n",
    "\n",
    "W2 = nd.random_normal(scale=weight_scale, shape=(num_hidden_1, num_hidden_1),ctx=ctx)\n",
    "b2 = nd.random_normal(scale=weight_scale, shape=num_hidden_1, ctx=ctx)\n",
    "\n",
    "W3 = nd.random_normal(scale=weight_scale, shape=(num_hidden_1, num_hidden_1),ctx=ctx)\n",
    "b3 = nd.random_normal(scale=weight_scale, shape=num_hidden_1, ctx=ctx)\n",
    "\n",
    "W4 = nd.random_normal(scale=weight_scale, shape=(num_hidden_1, num_outputs),ctx=ctx)\n",
    "b4 = nd.random_normal(scale=weight_scale, shape=num_outputs, ctx=ctx)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "params = [W1, b1, W2, b2,W3, b3,W4,b4]\n",
    "for param in params:\n",
    "    param.attach_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "[[-0.00552158 -0.00232112  0.00449515 ...,  0.00283108 -0.01035244\n",
       "  -0.00572781]\n",
       " [ 0.00114386 -0.0159209  -0.000143   ..., -0.00096905 -0.01225533\n",
       "  -0.01126637]\n",
       " [-0.00619978  0.00621978 -0.02401373 ..., -0.01500327  0.0113898\n",
       "   0.00543617]\n",
       " ..., \n",
       " [-0.00123413  0.00901198 -0.00683757 ...,  0.00033416 -0.00217825\n",
       "   0.01039872]\n",
       " [ 0.01067272  0.00987512  0.02066349 ...,  0.01227494  0.00369685\n",
       "   0.00487267]\n",
       " [-0.00065657 -0.00398379  0.01314763 ..., -0.00157058 -0.01120138\n",
       "  -0.00435292]]\n",
       "<NDArray 784x256 @cpu(0)>"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(yhat):\n",
    "    return nd.maximum(yhat, 0)\n",
    "\n",
    "def net(X):\n",
    "    X = X.reshape((-1, num_inputs))\n",
    "    h1 = relu(nd.dot(X, W1) + b1)\n",
    "    h2 = relu(nd.dot(h1, W2) + b2)\n",
    "    h3 = relu(nd.dot(h2, W3) + b3)\n",
    "    output = nd.dot(h3, W4) + b4\n",
    "    return output "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X, y = next(iter(train_data))\n",
    "# net(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_SM_CE = gluon.loss.SoftmaxCELoss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0. Loss: 2.030096, Train acc 0.195363, Test acc 0.439704\n",
      "Epoch 1. Loss: 1.013711, Train acc 0.591179, Test acc 0.764123\n",
      "Epoch 2. Loss: 0.661433, Train acc 0.743623, Test acc 0.716947\n",
      "Epoch 3. Loss: 0.523473, Train acc 0.799462, Test acc 0.814603\n",
      "Epoch 4. Loss: 0.468854, Train acc 0.823968, Test acc 0.834235\n",
      "Epoch 5. Loss: 0.429651, Train acc 0.839527, Test acc 0.837941\n",
      "Epoch 6. Loss: 0.398462, Train acc 0.851579, Test acc 0.854067\n",
      "Epoch 7. Loss: 0.526308, Train acc 0.814353, Test acc 0.843550\n",
      "Epoch 8. Loss: 0.390274, Train acc 0.855970, Test acc 0.861579\n",
      "Epoch 9. Loss: 0.364966, Train acc 0.865935, Test acc 0.861378\n",
      "Epoch 10. Loss: 0.350478, Train acc 0.869808, Test acc 0.868089\n",
      "Epoch 11. Loss: 0.341979, Train acc 0.873531, Test acc 0.835837\n",
      "Epoch 12. Loss: 0.325307, Train acc 0.880041, Test acc 0.875901\n",
      "Epoch 13. Loss: 0.316656, Train acc 0.881310, Test acc 0.878405\n",
      "Epoch 14. Loss: 0.307167, Train acc 0.886869, Test acc 0.876603\n",
      "Epoch 15. Loss: 0.299031, Train acc 0.888522, Test acc 0.882913\n",
      "Epoch 16. Loss: 0.294279, Train acc 0.890675, Test acc 0.870593\n",
      "Epoch 17. Loss: 0.285733, Train acc 0.893830, Test acc 0.887520\n",
      "Epoch 18. Loss: 0.278692, Train acc 0.896334, Test acc 0.879006\n",
      "Epoch 19. Loss: 0.271116, Train acc 0.899005, Test acc 0.877504\n",
      "Epoch 20. Loss: 0.265696, Train acc 0.900040, Test acc 0.881110\n",
      "Epoch 21. Loss: 0.263030, Train acc 0.900908, Test acc 0.888321\n",
      "Epoch 22. Loss: 0.257846, Train acc 0.903062, Test acc 0.877103\n",
      "Epoch 23. Loss: 0.252059, Train acc 0.904664, Test acc 0.885617\n",
      "Epoch 24. Loss: 0.258012, Train acc 0.903863, Test acc 0.887720\n",
      "Epoch 25. Loss: 0.251228, Train acc 0.906267, Test acc 0.877704\n",
      "Epoch 26. Loss: 0.238197, Train acc 0.909789, Test acc 0.891026\n",
      "Epoch 27. Loss: 0.233925, Train acc 0.912176, Test acc 0.890425\n",
      "Epoch 28. Loss: 0.231543, Train acc 0.912810, Test acc 0.886118\n",
      "Epoch 29. Loss: 0.227281, Train acc 0.914163, Test acc 0.888922\n",
      "Epoch 30. Loss: 0.220815, Train acc 0.916900, Test acc 0.885617\n",
      "Epoch 31. Loss: 0.219001, Train acc 0.917184, Test acc 0.860276\n",
      "Epoch 32. Loss: 0.255359, Train acc 0.907402, Test acc 0.893730\n",
      "Epoch 33. Loss: 0.219194, Train acc 0.917184, Test acc 0.890124\n",
      "Epoch 34. Loss: 0.212456, Train acc 0.919538, Test acc 0.883313\n",
      "Epoch 35. Loss: 0.207027, Train acc 0.920923, Test acc 0.888822\n",
      "Epoch 36. Loss: 0.214682, Train acc 0.918753, Test acc 0.888321\n",
      "Epoch 37. Loss: 0.201503, Train acc 0.923361, Test acc 0.891126\n",
      "Epoch 38. Loss: 0.201229, Train acc 0.923945, Test acc 0.885817\n",
      "Epoch 39. Loss: 0.195667, Train acc 0.925364, Test acc 0.894631\n",
      "Epoch 40. Loss: 0.192986, Train acc 0.926516, Test acc 0.885216\n",
      "Epoch 41. Loss: 0.191443, Train acc 0.927167, Test acc 0.894732\n",
      "Epoch 42. Loss: 0.185076, Train acc 0.929203, Test acc 0.893630\n",
      "Epoch 43. Loss: 0.180917, Train acc 0.930940, Test acc 0.892127\n",
      "Epoch 44. Loss: 0.182410, Train acc 0.930255, Test acc 0.895933\n",
      "Epoch 45. Loss: 0.175727, Train acc 0.932809, Test acc 0.889724\n",
      "Epoch 46. Loss: 0.176689, Train acc 0.932676, Test acc 0.896534\n",
      "Epoch 47. Loss: 0.171410, Train acc 0.936382, Test acc 0.891827\n",
      "Epoch 48. Loss: 0.170662, Train acc 0.934579, Test acc 0.898237\n",
      "Epoch 49. Loss: 0.168081, Train acc 0.935614, Test acc 0.899139\n",
      "Epoch 50. Loss: 0.163909, Train acc 0.937467, Test acc 0.877905\n",
      "Epoch 51. Loss: 0.161471, Train acc 0.938702, Test acc 0.884615\n"
     ]
    }
   ],
   "source": [
    "for e in range(epochs):\n",
    "    training_loss = 0\n",
    "    training_acc = 0\n",
    "    for datas, labels in train_data:\n",
    "        X = datas.as_in_context(ctx)\n",
    "        y = labels.as_in_context(ctx)\n",
    "        with ag.record():\n",
    "            output = net(X)\n",
    "            loss = loss_SM_CE(output, y)\n",
    "        loss.backward()\n",
    "        utils.SGD(params, learning_rate / batch_size)\n",
    "        training_loss += nd.mean(loss).asscalar()\n",
    "        training_acc += utils.accuracy(output, y)\n",
    "        \n",
    "#         print(W1)\n",
    "        \n",
    "    test_acc = utils.evaluate_accuracy(test_data, net)\n",
    "    print(\"Epoch %d. Loss: %f, Train acc %f, Test acc %f\" % (\n",
    "        e, training_loss/len(train_data),\n",
    "        training_acc/len(train_data), test_acc))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
