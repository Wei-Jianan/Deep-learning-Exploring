{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-20T18:10:33.423089Z",
     "start_time": "2018-04-20T18:10:32.479287Z"
    }
   },
   "outputs": [],
   "source": [
    "from mxnet import nd, gluon\n",
    "from mxnet import autograd as ag\n",
    "import mxnet as mx\n",
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-20T18:10:33.428755Z",
     "start_time": "2018-04-20T18:10:33.425886Z"
    }
   },
   "outputs": [],
   "source": [
    "batch_size = 256\n",
    "epochs = 50\n",
    "weight_scale = 0.01\n",
    "learning_rate = 0.2\n",
    "ctx = utils.try_gpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-20T18:10:33.939255Z",
     "start_time": "2018-04-20T18:10:33.430618Z"
    }
   },
   "outputs": [],
   "source": [
    "train_data, test_data = utils.load_data_fashion_mnist(batch_size)\n",
    "\n",
    "W1 = nd.random_normal(shape=(20, 1, 5, 5), scale=weight_scale, ctx=ctx)\n",
    "b1 = nd.zeros(W1.shape[0], ctx=ctx)\n",
    "\n",
    "W2 = nd.random_normal(shape=(50, 20, 3, 3), scale=weight_scale, ctx=ctx)\n",
    "b2 = nd.zeros(W2.shape[0], ctx=ctx)\n",
    "\n",
    "W3 = nd.random_normal(shape=(1250, 128), scale=weight_scale, ctx=ctx)\n",
    "b3 = nd.zeros(W3.shape[1], ctx=ctx)\n",
    "\n",
    "W4 = nd.random_normal(shape=(128, 10), scale=weight_scale, ctx=ctx)\n",
    "b4 = nd.zeros(shape=W4.shape[1], ctx=ctx)\n",
    "\n",
    "paramas = [W1,b1,W2,b2,W3,b3,W4,b4]\n",
    "\n",
    "for param in paramas:\n",
    "    param.attach_grad()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-20T18:10:33.946764Z",
     "start_time": "2018-04-20T18:10:33.941294Z"
    }
   },
   "outputs": [],
   "source": [
    "def net(X, verbose=False):\n",
    "#     X = X.as_in_context(ctx)\n",
    "    print('X shape:', X.shape) if verbose else None\n",
    "    h1_conv = nd.Convolution(data=X, weight=W1, bias=b1, kernel=W1.shape[2:],num_filter=W1.shape[0])\n",
    "    print('h1 convolution shape:', h1_conv.shape) if verbose else None\n",
    "    h1_relu = nd.relu(h1_conv)\n",
    "    h1 = nd.Pooling(data=h1_relu, pool_type='max', kernel=(2, 2), stride=(2,2) )\n",
    "    print('h1 polling shape:', h1.shape) if verbose else None\n",
    "    \n",
    "    h2_conv = nd.Convolution(data=h1, weight=W2, bias=b2, kernel=W2.shape[2:], num_filter=W2.shape[0])\n",
    "    print('h2 convolution shape:', h2_conv.shape) if verbose else None\n",
    "    h2_relu = nd.relu(h2_conv)\n",
    "    h2 = nd.Pooling(data=h2_relu, pool_type='max', kernel=(2, 2), stride=(2, 2))\n",
    "    print('h2 polling shape:', h2_relu.shape) if verbose else None\n",
    "    h2 = h2.flatten()\n",
    "    print('h2 flatten shape:', h2.shape) if verbose else None\n",
    "    \n",
    "    \n",
    "    h3_linear = nd.dot(h2, W3) + b3\n",
    "    h3 = nd.relu(h3_linear)\n",
    "    print('h3 shape:', h3.shape) if verbose else None\n",
    "    \n",
    "    h4_linear = nd.dot(h3, W4) + b4\n",
    "    print('h4  shape:', h4_linear.shape) if verbose else None\n",
    "    \n",
    "    return h4_linear\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-20T18:10:34.328319Z",
     "start_time": "2018-04-20T18:10:33.949487Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape: (256, 1, 28, 28)\n",
      "h1 convolution shape: (256, 20, 24, 24)\n",
      "h1 polling shape: (256, 20, 12, 12)\n",
      "h2 convolution shape: (256, 50, 10, 10)\n",
      "h2 polling shape: (256, 50, 10, 10)\n",
      "h2 flatten shape: (256, 1250)\n",
      "h3 shape: (256, 128)\n",
      "h4  shape: (256, 10)\n",
      "\n",
      "[[  9.00049599e-06  -6.47684237e-06  -7.64564320e-05 ...,  -5.99803170e-05\n",
      "   -3.85270323e-05   2.01009007e-05]\n",
      " [ -3.44139553e-05  -3.96617907e-06  -8.17592081e-05 ...,   1.81698288e-05\n",
      "   -7.72076601e-05   2.07855373e-05]\n",
      " [ -2.88460888e-05   1.78134687e-05  -8.40852008e-05 ...,  -4.27798586e-05\n",
      "   -6.68772263e-05   1.48699255e-05]\n",
      " ..., \n",
      " [ -2.75682578e-05   7.16838349e-06  -9.73502974e-05 ...,  -5.10061618e-05\n",
      "   -3.83101105e-05   2.92994196e-06]\n",
      " [ -8.35243191e-05   2.21156170e-05  -8.32318474e-05 ...,  -9.82839765e-07\n",
      "   -9.14775592e-05   2.03187465e-05]\n",
      " [ -6.66341439e-05   4.32687739e-06  -6.81044985e-05 ...,  -3.81039354e-05\n",
      "   -7.40362884e-05   1.87062869e-05]]\n",
      "<NDArray 256x10 @cpu(0)>\n"
     ]
    }
   ],
   "source": [
    "for X, _ in train_data:\n",
    "    print(net(X, verbose=True))\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-04-20T18:10:32.464Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0. Loss: 2.302227, Train acc 0.107873, Test acc 0.286859\n",
      "Epoch 1. Loss: 1.475448, Train acc 0.446748, Test acc 0.652444\n",
      "Epoch 2. Loss: 0.638800, Train acc 0.749666, Test acc 0.778245\n",
      "Epoch 3. Loss: 0.509663, Train acc 0.803786, Test acc 0.835537\n",
      "Epoch 4. Loss: 0.452598, Train acc 0.831130, Test acc 0.833333\n",
      "Epoch 5. Loss: 0.412683, Train acc 0.850361, Test acc 0.848157\n",
      "Epoch 6. Loss: 0.378172, Train acc 0.860243, Test acc 0.869291\n",
      "Epoch 7. Loss: 0.353821, Train acc 0.870793, Test acc 0.875501\n",
      "Epoch 8. Loss: 0.338102, Train acc 0.876536, Test acc 0.867989\n",
      "Epoch 9. Loss: 0.326730, Train acc 0.880292, Test acc 0.879908\n",
      "Epoch 10. Loss: 0.311558, Train acc 0.885116, Test acc 0.872696\n",
      "Epoch 11. Loss: 0.303004, Train acc 0.887987, Test acc 0.865986\n",
      "Epoch 12. Loss: 0.296622, Train acc 0.889573, Test acc 0.884615\n"
     ]
    }
   ],
   "source": [
    "loss_SF_CE = gluon.loss.SoftmaxCrossEntropyLoss()\n",
    "\n",
    "for e in range(epochs):\n",
    "    train_loss = 0\n",
    "    train_acc = 0\n",
    "    for X, y in train_data:\n",
    "        X = X.as_in_context(ctx)\n",
    "        y = y.as_in_context(ctx)\n",
    "        with ag.record():\n",
    "            output = net(X)\n",
    "            loss = loss_SF_CE(output, y)\n",
    "        loss.backward()\n",
    "        utils.SGD(paramas, learning_rate/ batch_size)\n",
    "        \n",
    "        train_loss += loss.mean().asscalar()\n",
    "        train_acc += utils.accuracy(output, y)\n",
    "    test_acc = utils.evaluate_accuracy(test_data, net)\n",
    "    print(\"Epoch %d. Loss: %f, Train acc %f, Test acc %f\" % (\n",
    "    e, train_loss/len(train_data),\n",
    "    train_acc/len(train_data), test_acc))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-20T17:03:16.680262Z",
     "start_time": "2018-04-20T17:03:16.676518Z"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
