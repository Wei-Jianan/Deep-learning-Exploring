{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\weij\\anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import sys\n",
    "sys.path.insert(0, '..')\n",
    "import gluonbook as gb\n",
    "from mxnet import nd, init, gluon\n",
    "from mxnet.gluon import nn\n",
    "\n",
    "class Inception(nn.Block):\n",
    "    # c1 - c4 为每条线路里的层的输出通道数。\n",
    "    def __init__(self, c1, c2, c3, c4, **kwargs):\n",
    "        super(Inception, self).__init__(**kwargs)\n",
    "        # 线路 1，单 1 x 1 卷积层。\n",
    "        self.p1_1 = nn.Conv2D(c1, kernel_size=1, activation='relu')\n",
    "        # 线路 2，1 x 1 卷积层后接 3 x 3 卷积层。\n",
    "        self.p2_1 = nn.Conv2D(c2[0], kernel_size=1, activation='relu')\n",
    "        self.p2_2 = nn.Conv2D(c2[1], kernel_size=3, padding=1,\n",
    "                              activation='relu')\n",
    "        # 线路 3，1 x 1 卷积层后接 5 x 5 卷积层。\n",
    "        self.p3_1 = nn.Conv2D(c3[0], kernel_size=1, activation='relu')\n",
    "        self.p3_2 = nn.Conv2D(c3[1], kernel_size=5, padding=2,\n",
    "                              activation='relu')\n",
    "        # 线路 4，3 x 3 最大池化层后接 1 x 1 卷积层。\n",
    "        self.p4_1 = nn.MaxPool2D(pool_size=3, strides=1, padding=1)\n",
    "        self.p4_2 = nn.Conv2D(c4, kernel_size=1, activation='relu')\n",
    "\n",
    "    def forward(self, x):\n",
    "        p1 = self.p1_1(x)\n",
    "        p2 = self.p2_2(self.p2_1(x))\n",
    "        p3 = self.p3_2(self.p3_1(x))\n",
    "        p4 = self.p4_2(self.p4_1(x))\n",
    "        # 在通道维上合并输出\n",
    "        return nd.concat(p1, p2, p3, p4, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "b1 = nn.Sequential()\n",
    "b1.add(\n",
    "    nn.Conv2D(64, kernel_size=7, strides=2, padding=3, activation='relu'),\n",
    "    nn.MaxPool2D(pool_size=3, strides=2, padding=1)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "b2 = nn.Sequential()\n",
    "b2.add(\n",
    "    nn.Conv2D(64, kernel_size=1),\n",
    "    nn.Conv2D(192, kernel_size=3, padding=1),\n",
    "    nn.MaxPool2D(pool_size=3, strides=2, padding=1)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "b3 = nn.Sequential()\n",
    "b3.add(\n",
    "    Inception(64, (96, 128), (16, 32), 32),\n",
    "    Inception(128, (128, 192), (32, 96), 64),\n",
    "    nn.MaxPool2D(pool_size=3, strides=2, padding=1)\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "b4 = nn.Sequential()\n",
    "b4.add(\n",
    "    Inception(192, (96, 208), (16, 48), 64),\n",
    "    Inception(160, (112, 224), (24, 64), 64),\n",
    "    Inception(128, (128, 256), (24, 64), 64),\n",
    "    Inception(112, (144, 288), (32, 64), 64),\n",
    "    Inception(256, (160, 320), (32, 128), 128),\n",
    "    nn.MaxPool2D(pool_size=3, strides=2, padding=1)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "b5 = nn.Sequential()\n",
    "b5.add(\n",
    "    Inception(256, (160, 320), (32, 128), 128),\n",
    "    Inception(384, (192, 384), (48, 128), 128),\n",
    "    nn.GlobalAvgPool2D()\n",
    ")\n",
    "\n",
    "net = nn.Sequential()\n",
    "net.add(b1, b2, b3, b4, b5, nn.Dense(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ctx = gb.try_gpu()\n",
    "# X = nd.random.uniform(shape=(1,1,96,96), ctx=ctx)\n",
    "# net.initialize( ctx=ctx, init=init.Xavier())\n",
    "# for layer in net:\n",
    "#     X = layer(X)\n",
    "#     print(layer.name, 'output shape:\\t', X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training on gpu(0)\n",
      "epoch 1, loss 2.0780, train acc 0.231, test acc 0.568, time 423.1 sec\n",
      "epoch 2, loss 1.0638, train acc 0.589, test acc 0.742, time 414.0 sec\n",
      "epoch 3, loss 0.6144, train acc 0.768, test acc 0.825, time 414.6 sec\n",
      "epoch 4, loss 0.6369, train acc 0.769, test acc 0.699, time 411.1 sec\n",
      "epoch 5, loss 0.4945, train acc 0.816, test acc 0.800, time 400.7 sec\n",
      "epoch 6, loss 0.3922, train acc 0.853, test acc 0.871, time 413.9 sec\n",
      "epoch 7, loss 0.3508, train acc 0.869, test acc 0.874, time 414.2 sec\n",
      "epoch 8, loss 0.3257, train acc 0.878, test acc 0.871, time 414.3 sec\n",
      "epoch 9, loss 0.4024, train acc 0.856, test acc 0.852, time 414.6 sec\n",
      "epoch 10, loss 0.3160, train acc 0.881, test acc 0.896, time 414.7 sec\n",
      "epoch 11, loss 0.2793, train acc 0.897, test acc 0.899, time 414.9 sec\n",
      "epoch 12, loss 0.2634, train acc 0.902, test acc 0.906, time 414.3 sec\n",
      "epoch 13, loss 0.2483, train acc 0.908, test acc 0.903, time 414.6 sec\n",
      "epoch 14, loss 0.2378, train acc 0.911, test acc 0.898, time 414.3 sec\n",
      "epoch 15, loss 0.2256, train acc 0.916, test acc 0.913, time 413.4 sec\n",
      "epoch 16, loss 0.2140, train acc 0.921, test acc 0.910, time 414.1 sec\n",
      "epoch 17, loss 0.2053, train acc 0.923, test acc 0.921, time 414.2 sec\n",
      "epoch 18, loss 104413660823.7786, train acc 0.247, test acc 0.101, time 411.2 sec\n",
      "epoch 19, loss 2.3029, train acc 0.098, test acc 0.100, time 410.6 sec\n",
      "epoch 20, loss 2.3029, train acc 0.100, test acc 0.100, time 411.2 sec\n",
      "epoch 21, loss 2.3028, train acc 0.099, test acc 0.100, time 411.5 sec\n",
      "epoch 22, loss 2.3028, train acc 0.099, test acc 0.100, time 410.9 sec\n",
      "epoch 23, loss 2.3030, train acc 0.099, test acc 0.100, time 410.5 sec\n",
      "epoch 24, loss 2.3029, train acc 0.099, test acc 0.099, time 410.6 sec\n",
      "epoch 25, loss 2.3029, train acc 0.098, test acc 0.100, time 410.8 sec\n",
      "epoch 26, loss 2.3029, train acc 0.100, test acc 0.100, time 410.9 sec\n",
      "epoch 27, loss 2.3029, train acc 0.099, test acc 0.099, time 411.0 sec\n",
      "epoch 28, loss 2.3029, train acc 0.100, test acc 0.100, time 410.7 sec\n",
      "epoch 29, loss 2.3028, train acc 0.101, test acc 0.100, time 410.6 sec\n",
      "epoch 30, loss 2.3029, train acc 0.096, test acc 0.100, time 410.8 sec\n",
      "epoch 31, loss 2.3029, train acc 0.096, test acc 0.100, time 410.6 sec\n",
      "epoch 32, loss 2.3029, train acc 0.098, test acc 0.101, time 410.9 sec\n",
      "epoch 33, loss 2.3029, train acc 0.098, test acc 0.100, time 410.6 sec\n",
      "epoch 34, loss 2.3029, train acc 0.099, test acc 0.100, time 410.7 sec\n",
      "epoch 35, loss 2.3029, train acc 0.099, test acc 0.100, time 411.0 sec\n",
      "epoch 36, loss 2.3029, train acc 0.098, test acc 0.100, time 410.8 sec\n",
      "epoch 37, loss 2.3028, train acc 0.099, test acc 0.100, time 411.0 sec\n",
      "epoch 38, loss 2.3029, train acc 0.099, test acc 0.099, time 410.7 sec\n",
      "epoch 39, loss 2.3029, train acc 0.098, test acc 0.100, time 410.8 sec\n",
      "epoch 40, loss 2.3029, train acc 0.099, test acc 0.099, time 411.0 sec\n",
      "epoch 41, loss 2.3029, train acc 0.098, test acc 0.100, time 411.1 sec\n",
      "epoch 42, loss 2.3029, train acc 0.099, test acc 0.100, time 410.4 sec\n",
      "epoch 43, loss 2.3029, train acc 0.099, test acc 0.100, time 410.8 sec\n",
      "epoch 44, loss 2.3029, train acc 0.097, test acc 0.101, time 410.6 sec\n",
      "epoch 45, loss 2.3029, train acc 0.101, test acc 0.099, time 410.5 sec\n",
      "epoch 46, loss 2.3029, train acc 0.099, test acc 0.100, time 410.7 sec\n",
      "epoch 47, loss 2.3029, train acc 0.099, test acc 0.100, time 410.4 sec\n",
      "epoch 48, loss 2.3029, train acc 0.098, test acc 0.100, time 411.2 sec\n",
      "epoch 49, loss 2.3029, train acc 0.099, test acc 0.100, time 410.8 sec\n",
      "epoch 50, loss 2.3029, train acc 0.099, test acc 0.100, time 411.1 sec\n",
      "epoch 51, loss 2.3028, train acc 0.098, test acc 0.101, time 410.4 sec\n",
      "epoch 52, loss 2.3029, train acc 0.100, test acc 0.100, time 410.9 sec\n",
      "epoch 53, loss 2.3029, train acc 0.100, test acc 0.101, time 411.0 sec\n",
      "epoch 54, loss 2.3029, train acc 0.099, test acc 0.100, time 411.0 sec\n",
      "epoch 55, loss 2.3028, train acc 0.101, test acc 0.101, time 410.7 sec\n",
      "epoch 56, loss 2.3029, train acc 0.100, test acc 0.100, time 411.0 sec\n",
      "epoch 57, loss 2.3029, train acc 0.098, test acc 0.100, time 410.8 sec\n",
      "epoch 58, loss 2.3029, train acc 0.101, test acc 0.100, time 410.7 sec\n",
      "epoch 59, loss 2.3029, train acc 0.100, test acc 0.100, time 410.4 sec\n",
      "epoch 60, loss 2.3029, train acc 0.099, test acc 0.101, time 410.8 sec\n",
      "epoch 61, loss 2.3029, train acc 0.099, test acc 0.101, time 411.1 sec\n",
      "epoch 62, loss 2.3029, train acc 0.100, test acc 0.100, time 410.9 sec\n",
      "epoch 63, loss 2.3029, train acc 0.100, test acc 0.100, time 411.0 sec\n",
      "epoch 64, loss 2.3028, train acc 0.100, test acc 0.100, time 410.8 sec\n",
      "epoch 65, loss 2.3028, train acc 0.100, test acc 0.100, time 410.9 sec\n",
      "epoch 66, loss 2.3028, train acc 0.099, test acc 0.099, time 410.9 sec\n",
      "epoch 67, loss 2.3029, train acc 0.099, test acc 0.101, time 410.9 sec\n",
      "epoch 68, loss 2.3029, train acc 0.098, test acc 0.099, time 410.8 sec\n",
      "epoch 69, loss 2.3029, train acc 0.099, test acc 0.101, time 410.8 sec\n",
      "epoch 70, loss 2.3029, train acc 0.097, test acc 0.099, time 411.1 sec\n",
      "epoch 71, loss 2.3029, train acc 0.098, test acc 0.100, time 410.9 sec\n",
      "epoch 72, loss 2.3029, train acc 0.099, test acc 0.100, time 410.8 sec\n",
      "epoch 73, loss 2.3029, train acc 0.099, test acc 0.100, time 411.0 sec\n",
      "epoch 74, loss 2.3028, train acc 0.097, test acc 0.100, time 411.3 sec\n",
      "epoch 75, loss 2.3028, train acc 0.099, test acc 0.099, time 410.6 sec\n",
      "epoch 76, loss 2.3028, train acc 0.100, test acc 0.100, time 411.2 sec\n",
      "epoch 77, loss 2.3029, train acc 0.098, test acc 0.099, time 410.8 sec\n",
      "epoch 78, loss 2.3029, train acc 0.099, test acc 0.099, time 410.7 sec\n",
      "epoch 79, loss 2.3028, train acc 0.100, test acc 0.100, time 410.9 sec\n",
      "epoch 80, loss 2.3029, train acc 0.099, test acc 0.100, time 411.1 sec\n",
      "epoch 81, loss 2.3029, train acc 0.101, test acc 0.100, time 411.2 sec\n",
      "epoch 82, loss 2.3029, train acc 0.098, test acc 0.101, time 411.0 sec\n",
      "epoch 83, loss 2.3029, train acc 0.099, test acc 0.100, time 411.4 sec\n",
      "epoch 84, loss 2.3029, train acc 0.099, test acc 0.100, time 410.9 sec\n",
      "epoch 85, loss 2.3029, train acc 0.099, test acc 0.101, time 411.0 sec\n",
      "epoch 86, loss 2.3029, train acc 0.098, test acc 0.100, time 410.9 sec\n",
      "epoch 87, loss 2.3029, train acc 0.099, test acc 0.101, time 411.0 sec\n",
      "epoch 88, loss 2.3028, train acc 0.099, test acc 0.100, time 410.8 sec\n",
      "epoch 89, loss 2.3028, train acc 0.101, test acc 0.101, time 410.8 sec\n",
      "epoch 90, loss 2.3029, train acc 0.098, test acc 0.100, time 411.0 sec\n",
      "epoch 91, loss 2.3028, train acc 0.099, test acc 0.100, time 410.8 sec\n",
      "epoch 92, loss 2.3029, train acc 0.097, test acc 0.101, time 411.0 sec\n",
      "epoch 93, loss 2.3029, train acc 0.098, test acc 0.101, time 410.9 sec\n",
      "epoch 94, loss 2.3028, train acc 0.098, test acc 0.100, time 410.9 sec\n",
      "epoch 95, loss 2.3028, train acc 0.100, test acc 0.100, time 411.1 sec\n",
      "epoch 96, loss 2.3029, train acc 0.097, test acc 0.100, time 410.8 sec\n",
      "epoch 97, loss 2.3028, train acc 0.100, test acc 0.100, time 411.0 sec\n",
      "epoch 98, loss 2.3028, train acc 0.102, test acc 0.099, time 410.6 sec\n",
      "epoch 99, loss 2.3029, train acc 0.099, test acc 0.100, time 410.7 sec\n",
      "epoch 100, loss 2.3029, train acc 0.097, test acc 0.100, time 410.9 sec\n",
      "epoch 101, loss 2.3029, train acc 0.100, test acc 0.100, time 410.7 sec\n",
      "epoch 102, loss 2.3028, train acc 0.098, test acc 0.100, time 410.7 sec\n",
      "epoch 103, loss 2.3029, train acc 0.100, test acc 0.100, time 410.6 sec\n",
      "epoch 104, loss 2.3030, train acc 0.098, test acc 0.101, time 410.7 sec\n",
      "epoch 105, loss 2.3028, train acc 0.100, test acc 0.100, time 411.0 sec\n",
      "epoch 106, loss 2.3028, train acc 0.101, test acc 0.101, time 410.3 sec\n",
      "epoch 107, loss 2.3029, train acc 0.099, test acc 0.099, time 411.0 sec\n",
      "epoch 108, loss 2.3028, train acc 0.099, test acc 0.099, time 410.6 sec\n",
      "epoch 109, loss 2.3029, train acc 0.100, test acc 0.101, time 411.1 sec\n",
      "epoch 110, loss 2.3028, train acc 0.100, test acc 0.100, time 410.7 sec\n",
      "epoch 111, loss 2.3029, train acc 0.099, test acc 0.100, time 410.7 sec\n",
      "epoch 112, loss 2.3028, train acc 0.100, test acc 0.100, time 409.8 sec\n",
      "epoch 113, loss 2.3028, train acc 0.099, test acc 0.100, time 411.0 sec\n",
      "epoch 114, loss 2.3029, train acc 0.097, test acc 0.100, time 410.9 sec\n",
      "epoch 115, loss 2.3029, train acc 0.100, test acc 0.099, time 410.8 sec\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 116, loss 2.3029, train acc 0.099, test acc 0.101, time 410.7 sec\n",
      "epoch 117, loss 2.3029, train acc 0.099, test acc 0.100, time 410.3 sec\n",
      "epoch 118, loss 2.3028, train acc 0.100, test acc 0.099, time 410.7 sec\n",
      "epoch 119, loss 2.3029, train acc 0.099, test acc 0.100, time 410.7 sec\n",
      "epoch 120, loss 2.3029, train acc 0.099, test acc 0.101, time 410.9 sec\n",
      "epoch 121, loss 2.3029, train acc 0.098, test acc 0.099, time 410.8 sec\n",
      "epoch 122, loss 2.3029, train acc 0.098, test acc 0.100, time 410.7 sec\n",
      "epoch 123, loss 2.3029, train acc 0.100, test acc 0.099, time 411.0 sec\n",
      "epoch 124, loss 2.3029, train acc 0.099, test acc 0.100, time 411.1 sec\n",
      "epoch 125, loss 2.3029, train acc 0.099, test acc 0.100, time 410.5 sec\n",
      "epoch 126, loss 2.3029, train acc 0.096, test acc 0.100, time 410.8 sec\n",
      "epoch 127, loss 2.3029, train acc 0.099, test acc 0.100, time 411.1 sec\n",
      "epoch 128, loss 2.3028, train acc 0.100, test acc 0.100, time 410.9 sec\n",
      "epoch 129, loss 2.3028, train acc 0.099, test acc 0.100, time 410.8 sec\n",
      "epoch 130, loss 2.3029, train acc 0.099, test acc 0.100, time 410.9 sec\n",
      "epoch 131, loss 2.3028, train acc 0.100, test acc 0.100, time 410.8 sec\n",
      "epoch 132, loss 2.3028, train acc 0.100, test acc 0.100, time 411.2 sec\n",
      "epoch 133, loss 2.3029, train acc 0.098, test acc 0.100, time 410.7 sec\n",
      "epoch 134, loss 2.3029, train acc 0.099, test acc 0.100, time 410.6 sec\n",
      "epoch 135, loss 2.3029, train acc 0.098, test acc 0.100, time 410.7 sec\n",
      "epoch 136, loss 2.3028, train acc 0.099, test acc 0.100, time 411.0 sec\n",
      "epoch 137, loss 2.3028, train acc 0.100, test acc 0.100, time 410.9 sec\n",
      "epoch 138, loss 2.3029, train acc 0.098, test acc 0.100, time 411.0 sec\n",
      "epoch 139, loss 2.3029, train acc 0.098, test acc 0.100, time 410.6 sec\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-cc504eb8abbc>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgb\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_data_fashion_mnist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m170\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m224\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgluon\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSoftmaxCrossEntropyLoss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[0mgb\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnet\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mctx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m150\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Downloads\\gluon-tutorials-zh\\gluonbook\\utils.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(train_iter, test_iter, net, loss, trainer, ctx, num_epochs, print_batches)\u001b[0m\n\u001b[0;32m    187\u001b[0m                 \u001b[0ml\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    188\u001b[0m             train_acc_sum += sum([(y_hat.argmax(axis=1) == y).sum().asscalar()\n\u001b[1;32m--> 189\u001b[1;33m                                  for y_hat, y in zip(y_hats, ys)])\n\u001b[0m\u001b[0;32m    190\u001b[0m             \u001b[0mtrain_l_sum\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0ml\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masscalar\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0ml\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mls\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    191\u001b[0m             \u001b[0mtrainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Downloads\\gluon-tutorials-zh\\gluonbook\\utils.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    187\u001b[0m                 \u001b[0ml\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    188\u001b[0m             train_acc_sum += sum([(y_hat.argmax(axis=1) == y).sum().asscalar()\n\u001b[1;32m--> 189\u001b[1;33m                                  for y_hat, y in zip(y_hats, ys)])\n\u001b[0m\u001b[0;32m    190\u001b[0m             \u001b[0mtrain_l_sum\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0ml\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masscalar\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0ml\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mls\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    191\u001b[0m             \u001b[0mtrainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\mxnet\\ndarray\\ndarray.py\u001b[0m in \u001b[0;36masscalar\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1892\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1893\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"The current array is not a scalar\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1894\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1895\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1896\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\mxnet\\ndarray\\ndarray.py\u001b[0m in \u001b[0;36masnumpy\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1874\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1875\u001b[0m             \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mctypes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata_as\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mctypes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mc_void_p\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1876\u001b[1;33m             ctypes.c_size_t(data.size)))\n\u001b[0m\u001b[0;32m   1877\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1878\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "lr = 0.1\n",
    "ctx = gb.try_gpu()\n",
    "net.initialize( ctx=ctx, init=init.Xavier())\n",
    "trainer = gluon.Trainer(net.collect_params(), 'sgd', {'learning_rate': lr})\n",
    "train_data, test_data = gb.load_data_fashion_mnist(batch_size=170, resize=224)\n",
    "\n",
    "loss = gluon.loss.SoftmaxCrossEntropyLoss()\n",
    "gb.train(train_data, test_data, net, loss, trainer, ctx, num_epochs=150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "for X, y in test_data:\n",
    "    break\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 1, 224, 224)\n",
      "\n",
      "[[[[nan]]\n",
      "\n",
      "  [[nan]]\n",
      "\n",
      "  [[nan]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[nan]]\n",
      "\n",
      "  [[nan]]\n",
      "\n",
      "  [[nan]]]\n",
      "\n",
      "\n",
      " [[[nan]]\n",
      "\n",
      "  [[nan]]\n",
      "\n",
      "  [[nan]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[nan]]\n",
      "\n",
      "  [[nan]]\n",
      "\n",
      "  [[nan]]]\n",
      "\n",
      "\n",
      " [[[nan]]\n",
      "\n",
      "  [[nan]]\n",
      "\n",
      "  [[nan]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[nan]]\n",
      "\n",
      "  [[nan]]\n",
      "\n",
      "  [[nan]]]\n",
      "\n",
      "\n",
      " ...\n",
      "\n",
      "\n",
      " [[[nan]]\n",
      "\n",
      "  [[nan]]\n",
      "\n",
      "  [[nan]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[nan]]\n",
      "\n",
      "  [[nan]]\n",
      "\n",
      "  [[nan]]]\n",
      "\n",
      "\n",
      " [[[nan]]\n",
      "\n",
      "  [[nan]]\n",
      "\n",
      "  [[nan]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[nan]]\n",
      "\n",
      "  [[nan]]\n",
      "\n",
      "  [[nan]]]\n",
      "\n",
      "\n",
      " [[[nan]]\n",
      "\n",
      "  [[nan]]\n",
      "\n",
      "  [[nan]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[nan]]\n",
      "\n",
      "  [[nan]]\n",
      "\n",
      "  [[nan]]]]\n",
      "<NDArray 64x64x1x1 @gpu(0)>\n",
      "\n",
      "[[[[nan nan nan]\n",
      "   [nan nan nan]\n",
      "   [nan nan nan]]\n",
      "\n",
      "  [[nan nan nan]\n",
      "   [nan nan nan]\n",
      "   [nan nan nan]]\n",
      "\n",
      "  [[nan nan nan]\n",
      "   [nan nan nan]\n",
      "   [nan nan nan]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[nan nan nan]\n",
      "   [nan nan nan]\n",
      "   [nan nan nan]]\n",
      "\n",
      "  [[nan nan nan]\n",
      "   [nan nan nan]\n",
      "   [nan nan nan]]\n",
      "\n",
      "  [[nan nan nan]\n",
      "   [nan nan nan]\n",
      "   [nan nan nan]]]\n",
      "\n",
      "\n",
      " [[[nan nan nan]\n",
      "   [nan nan nan]\n",
      "   [nan nan nan]]\n",
      "\n",
      "  [[nan nan nan]\n",
      "   [nan nan nan]\n",
      "   [nan nan nan]]\n",
      "\n",
      "  [[nan nan nan]\n",
      "   [nan nan nan]\n",
      "   [nan nan nan]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[nan nan nan]\n",
      "   [nan nan nan]\n",
      "   [nan nan nan]]\n",
      "\n",
      "  [[nan nan nan]\n",
      "   [nan nan nan]\n",
      "   [nan nan nan]]\n",
      "\n",
      "  [[nan nan nan]\n",
      "   [nan nan nan]\n",
      "   [nan nan nan]]]\n",
      "\n",
      "\n",
      " [[[nan nan nan]\n",
      "   [nan nan nan]\n",
      "   [nan nan nan]]\n",
      "\n",
      "  [[nan nan nan]\n",
      "   [nan nan nan]\n",
      "   [nan nan nan]]\n",
      "\n",
      "  [[nan nan nan]\n",
      "   [nan nan nan]\n",
      "   [nan nan nan]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[nan nan nan]\n",
      "   [nan nan nan]\n",
      "   [nan nan nan]]\n",
      "\n",
      "  [[nan nan nan]\n",
      "   [nan nan nan]\n",
      "   [nan nan nan]]\n",
      "\n",
      "  [[nan nan nan]\n",
      "   [nan nan nan]\n",
      "   [nan nan nan]]]\n",
      "\n",
      "\n",
      " ...\n",
      "\n",
      "\n",
      " [[[nan nan nan]\n",
      "   [nan nan nan]\n",
      "   [nan nan nan]]\n",
      "\n",
      "  [[nan nan nan]\n",
      "   [nan nan nan]\n",
      "   [nan nan nan]]\n",
      "\n",
      "  [[nan nan nan]\n",
      "   [nan nan nan]\n",
      "   [nan nan nan]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[nan nan nan]\n",
      "   [nan nan nan]\n",
      "   [nan nan nan]]\n",
      "\n",
      "  [[nan nan nan]\n",
      "   [nan nan nan]\n",
      "   [nan nan nan]]\n",
      "\n",
      "  [[nan nan nan]\n",
      "   [nan nan nan]\n",
      "   [nan nan nan]]]\n",
      "\n",
      "\n",
      " [[[nan nan nan]\n",
      "   [nan nan nan]\n",
      "   [nan nan nan]]\n",
      "\n",
      "  [[nan nan nan]\n",
      "   [nan nan nan]\n",
      "   [nan nan nan]]\n",
      "\n",
      "  [[nan nan nan]\n",
      "   [nan nan nan]\n",
      "   [nan nan nan]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[nan nan nan]\n",
      "   [nan nan nan]\n",
      "   [nan nan nan]]\n",
      "\n",
      "  [[nan nan nan]\n",
      "   [nan nan nan]\n",
      "   [nan nan nan]]\n",
      "\n",
      "  [[nan nan nan]\n",
      "   [nan nan nan]\n",
      "   [nan nan nan]]]\n",
      "\n",
      "\n",
      " [[[nan nan nan]\n",
      "   [nan nan nan]\n",
      "   [nan nan nan]]\n",
      "\n",
      "  [[nan nan nan]\n",
      "   [nan nan nan]\n",
      "   [nan nan nan]]\n",
      "\n",
      "  [[nan nan nan]\n",
      "   [nan nan nan]\n",
      "   [nan nan nan]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[nan nan nan]\n",
      "   [nan nan nan]\n",
      "   [nan nan nan]]\n",
      "\n",
      "  [[nan nan nan]\n",
      "   [nan nan nan]\n",
      "   [nan nan nan]]\n",
      "\n",
      "  [[nan nan nan]\n",
      "   [nan nan nan]\n",
      "   [nan nan nan]]]]\n",
      "<NDArray 192x64x3x3 @gpu(0)>\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'MaxPool2D' object has no attribute 'weight'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-82-4bc43b4da941>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;31m# layer = net[1]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0msub\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mlayer\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msub\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: 'MaxPool2D' object has no attribute 'weight'"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "x  = X[1:2].as_in_context(ctx)\n",
    "print(x.shape)\n",
    "# for layer in net:\n",
    "#     try:\n",
    "#         for sublayer in layer:\n",
    "#             print(sublayer.collect_params(),\"!!!!!\")\n",
    "#     except TypeError :\n",
    "#         print(sublayer)\n",
    "net.collect_params\n",
    "#     print(layer.name,\".................\", \":\\t\\t\", layer.collect_params(\".*weight\").get(\"conv15_weight\"))\n",
    "# layer = net[1]\n",
    "for sub in layer:\n",
    "    print(sub.weight.data())\n",
    "    nd.where"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Program Files\\Anaconda3\\envs\\ML\\lib\\site-packages\\mxnet\\gluon\\parameter.py:320: UserWarning: Parameter conv114_weight is already initialized, ignoring. Set force_reinit=True to re-initialize.\n",
      "  \"Set force_reinit=True to re-initialize.\"%self.name)\n",
      "C:\\Program Files\\Anaconda3\\envs\\ML\\lib\\site-packages\\mxnet\\gluon\\parameter.py:320: UserWarning: Parameter conv114_bias is already initialized, ignoring. Set force_reinit=True to re-initialize.\n",
      "  \"Set force_reinit=True to re-initialize.\"%self.name)\n",
      "C:\\Program Files\\Anaconda3\\envs\\ML\\lib\\site-packages\\mxnet\\gluon\\parameter.py:320: UserWarning: Parameter conv115_weight is already initialized, ignoring. Set force_reinit=True to re-initialize.\n",
      "  \"Set force_reinit=True to re-initialize.\"%self.name)\n",
      "C:\\Program Files\\Anaconda3\\envs\\ML\\lib\\site-packages\\mxnet\\gluon\\parameter.py:320: UserWarning: Parameter conv115_bias is already initialized, ignoring. Set force_reinit=True to re-initialize.\n",
      "  \"Set force_reinit=True to re-initialize.\"%self.name)\n",
      "C:\\Program Files\\Anaconda3\\envs\\ML\\lib\\site-packages\\mxnet\\gluon\\parameter.py:320: UserWarning: Parameter conv116_weight is already initialized, ignoring. Set force_reinit=True to re-initialize.\n",
      "  \"Set force_reinit=True to re-initialize.\"%self.name)\n",
      "C:\\Program Files\\Anaconda3\\envs\\ML\\lib\\site-packages\\mxnet\\gluon\\parameter.py:320: UserWarning: Parameter conv116_bias is already initialized, ignoring. Set force_reinit=True to re-initialize.\n",
      "  \"Set force_reinit=True to re-initialize.\"%self.name)\n",
      "C:\\Program Files\\Anaconda3\\envs\\ML\\lib\\site-packages\\mxnet\\gluon\\parameter.py:320: UserWarning: Parameter conv117_weight is already initialized, ignoring. Set force_reinit=True to re-initialize.\n",
      "  \"Set force_reinit=True to re-initialize.\"%self.name)\n",
      "C:\\Program Files\\Anaconda3\\envs\\ML\\lib\\site-packages\\mxnet\\gluon\\parameter.py:320: UserWarning: Parameter conv117_bias is already initialized, ignoring. Set force_reinit=True to re-initialize.\n",
      "  \"Set force_reinit=True to re-initialize.\"%self.name)\n",
      "C:\\Program Files\\Anaconda3\\envs\\ML\\lib\\site-packages\\mxnet\\gluon\\parameter.py:320: UserWarning: Parameter conv118_weight is already initialized, ignoring. Set force_reinit=True to re-initialize.\n",
      "  \"Set force_reinit=True to re-initialize.\"%self.name)\n",
      "C:\\Program Files\\Anaconda3\\envs\\ML\\lib\\site-packages\\mxnet\\gluon\\parameter.py:320: UserWarning: Parameter conv118_bias is already initialized, ignoring. Set force_reinit=True to re-initialize.\n",
      "  \"Set force_reinit=True to re-initialize.\"%self.name)\n",
      "C:\\Program Files\\Anaconda3\\envs\\ML\\lib\\site-packages\\mxnet\\gluon\\parameter.py:320: UserWarning: Parameter conv119_weight is already initialized, ignoring. Set force_reinit=True to re-initialize.\n",
      "  \"Set force_reinit=True to re-initialize.\"%self.name)\n",
      "C:\\Program Files\\Anaconda3\\envs\\ML\\lib\\site-packages\\mxnet\\gluon\\parameter.py:320: UserWarning: Parameter conv119_bias is already initialized, ignoring. Set force_reinit=True to re-initialize.\n",
      "  \"Set force_reinit=True to re-initialize.\"%self.name)\n",
      "C:\\Program Files\\Anaconda3\\envs\\ML\\lib\\site-packages\\mxnet\\gluon\\parameter.py:320: UserWarning: Parameter conv120_weight is already initialized, ignoring. Set force_reinit=True to re-initialize.\n",
      "  \"Set force_reinit=True to re-initialize.\"%self.name)\n",
      "C:\\Program Files\\Anaconda3\\envs\\ML\\lib\\site-packages\\mxnet\\gluon\\parameter.py:320: UserWarning: Parameter conv120_bias is already initialized, ignoring. Set force_reinit=True to re-initialize.\n",
      "  \"Set force_reinit=True to re-initialize.\"%self.name)\n",
      "C:\\Program Files\\Anaconda3\\envs\\ML\\lib\\site-packages\\mxnet\\gluon\\parameter.py:320: UserWarning: Parameter conv121_weight is already initialized, ignoring. Set force_reinit=True to re-initialize.\n",
      "  \"Set force_reinit=True to re-initialize.\"%self.name)\n",
      "C:\\Program Files\\Anaconda3\\envs\\ML\\lib\\site-packages\\mxnet\\gluon\\parameter.py:320: UserWarning: Parameter conv121_bias is already initialized, ignoring. Set force_reinit=True to re-initialize.\n",
      "  \"Set force_reinit=True to re-initialize.\"%self.name)\n",
      "C:\\Program Files\\Anaconda3\\envs\\ML\\lib\\site-packages\\mxnet\\gluon\\parameter.py:320: UserWarning: Parameter conv122_weight is already initialized, ignoring. Set force_reinit=True to re-initialize.\n",
      "  \"Set force_reinit=True to re-initialize.\"%self.name)\n",
      "C:\\Program Files\\Anaconda3\\envs\\ML\\lib\\site-packages\\mxnet\\gluon\\parameter.py:320: UserWarning: Parameter conv122_bias is already initialized, ignoring. Set force_reinit=True to re-initialize.\n",
      "  \"Set force_reinit=True to re-initialize.\"%self.name)\n",
      "C:\\Program Files\\Anaconda3\\envs\\ML\\lib\\site-packages\\mxnet\\gluon\\parameter.py:320: UserWarning: Parameter conv123_weight is already initialized, ignoring. Set force_reinit=True to re-initialize.\n",
      "  \"Set force_reinit=True to re-initialize.\"%self.name)\n",
      "C:\\Program Files\\Anaconda3\\envs\\ML\\lib\\site-packages\\mxnet\\gluon\\parameter.py:320: UserWarning: Parameter conv123_bias is already initialized, ignoring. Set force_reinit=True to re-initialize.\n",
      "  \"Set force_reinit=True to re-initialize.\"%self.name)\n",
      "C:\\Program Files\\Anaconda3\\envs\\ML\\lib\\site-packages\\mxnet\\gluon\\parameter.py:320: UserWarning: Parameter conv124_weight is already initialized, ignoring. Set force_reinit=True to re-initialize.\n",
      "  \"Set force_reinit=True to re-initialize.\"%self.name)\n",
      "C:\\Program Files\\Anaconda3\\envs\\ML\\lib\\site-packages\\mxnet\\gluon\\parameter.py:320: UserWarning: Parameter conv124_bias is already initialized, ignoring. Set force_reinit=True to re-initialize.\n",
      "  \"Set force_reinit=True to re-initialize.\"%self.name)\n",
      "C:\\Program Files\\Anaconda3\\envs\\ML\\lib\\site-packages\\mxnet\\gluon\\parameter.py:320: UserWarning: Parameter conv125_weight is already initialized, ignoring. Set force_reinit=True to re-initialize.\n",
      "  \"Set force_reinit=True to re-initialize.\"%self.name)\n",
      "C:\\Program Files\\Anaconda3\\envs\\ML\\lib\\site-packages\\mxnet\\gluon\\parameter.py:320: UserWarning: Parameter conv125_bias is already initialized, ignoring. Set force_reinit=True to re-initialize.\n",
      "  \"Set force_reinit=True to re-initialize.\"%self.name)\n",
      "C:\\Program Files\\Anaconda3\\envs\\ML\\lib\\site-packages\\mxnet\\gluon\\parameter.py:320: UserWarning: Parameter conv126_weight is already initialized, ignoring. Set force_reinit=True to re-initialize.\n",
      "  \"Set force_reinit=True to re-initialize.\"%self.name)\n",
      "C:\\Program Files\\Anaconda3\\envs\\ML\\lib\\site-packages\\mxnet\\gluon\\parameter.py:320: UserWarning: Parameter conv126_bias is already initialized, ignoring. Set force_reinit=True to re-initialize.\n",
      "  \"Set force_reinit=True to re-initialize.\"%self.name)\n",
      "C:\\Program Files\\Anaconda3\\envs\\ML\\lib\\site-packages\\mxnet\\gluon\\parameter.py:320: UserWarning: Parameter conv127_weight is already initialized, ignoring. Set force_reinit=True to re-initialize.\n",
      "  \"Set force_reinit=True to re-initialize.\"%self.name)\n",
      "C:\\Program Files\\Anaconda3\\envs\\ML\\lib\\site-packages\\mxnet\\gluon\\parameter.py:320: UserWarning: Parameter conv127_bias is already initialized, ignoring. Set force_reinit=True to re-initialize.\n",
      "  \"Set force_reinit=True to re-initialize.\"%self.name)\n",
      "C:\\Program Files\\Anaconda3\\envs\\ML\\lib\\site-packages\\mxnet\\gluon\\parameter.py:320: UserWarning: Parameter conv128_weight is already initialized, ignoring. Set force_reinit=True to re-initialize.\n",
      "  \"Set force_reinit=True to re-initialize.\"%self.name)\n",
      "C:\\Program Files\\Anaconda3\\envs\\ML\\lib\\site-packages\\mxnet\\gluon\\parameter.py:320: UserWarning: Parameter conv128_bias is already initialized, ignoring. Set force_reinit=True to re-initialize.\n",
      "  \"Set force_reinit=True to re-initialize.\"%self.name)\n",
      "C:\\Program Files\\Anaconda3\\envs\\ML\\lib\\site-packages\\mxnet\\gluon\\parameter.py:320: UserWarning: Parameter conv129_weight is already initialized, ignoring. Set force_reinit=True to re-initialize.\n",
      "  \"Set force_reinit=True to re-initialize.\"%self.name)\n",
      "C:\\Program Files\\Anaconda3\\envs\\ML\\lib\\site-packages\\mxnet\\gluon\\parameter.py:320: UserWarning: Parameter conv129_bias is already initialized, ignoring. Set force_reinit=True to re-initialize.\n",
      "  \"Set force_reinit=True to re-initialize.\"%self.name)\n",
      "C:\\Program Files\\Anaconda3\\envs\\ML\\lib\\site-packages\\mxnet\\gluon\\parameter.py:320: UserWarning: Parameter conv130_weight is already initialized, ignoring. Set force_reinit=True to re-initialize.\n",
      "  \"Set force_reinit=True to re-initialize.\"%self.name)\n",
      "C:\\Program Files\\Anaconda3\\envs\\ML\\lib\\site-packages\\mxnet\\gluon\\parameter.py:320: UserWarning: Parameter conv130_bias is already initialized, ignoring. Set force_reinit=True to re-initialize.\n",
      "  \"Set force_reinit=True to re-initialize.\"%self.name)\n",
      "C:\\Program Files\\Anaconda3\\envs\\ML\\lib\\site-packages\\mxnet\\gluon\\parameter.py:320: UserWarning: Parameter conv131_weight is already initialized, ignoring. Set force_reinit=True to re-initialize.\n",
      "  \"Set force_reinit=True to re-initialize.\"%self.name)\n",
      "C:\\Program Files\\Anaconda3\\envs\\ML\\lib\\site-packages\\mxnet\\gluon\\parameter.py:320: UserWarning: Parameter conv131_bias is already initialized, ignoring. Set force_reinit=True to re-initialize.\n",
      "  \"Set force_reinit=True to re-initialize.\"%self.name)\n",
      "C:\\Program Files\\Anaconda3\\envs\\ML\\lib\\site-packages\\mxnet\\gluon\\parameter.py:320: UserWarning: Parameter conv132_weight is already initialized, ignoring. Set force_reinit=True to re-initialize.\n",
      "  \"Set force_reinit=True to re-initialize.\"%self.name)\n",
      "C:\\Program Files\\Anaconda3\\envs\\ML\\lib\\site-packages\\mxnet\\gluon\\parameter.py:320: UserWarning: Parameter conv132_bias is already initialized, ignoring. Set force_reinit=True to re-initialize.\n",
      "  \"Set force_reinit=True to re-initialize.\"%self.name)\n",
      "C:\\Program Files\\Anaconda3\\envs\\ML\\lib\\site-packages\\mxnet\\gluon\\parameter.py:320: UserWarning: Parameter conv133_weight is already initialized, ignoring. Set force_reinit=True to re-initialize.\n",
      "  \"Set force_reinit=True to re-initialize.\"%self.name)\n",
      "C:\\Program Files\\Anaconda3\\envs\\ML\\lib\\site-packages\\mxnet\\gluon\\parameter.py:320: UserWarning: Parameter conv133_bias is already initialized, ignoring. Set force_reinit=True to re-initialize.\n",
      "  \"Set force_reinit=True to re-initialize.\"%self.name)\n",
      "C:\\Program Files\\Anaconda3\\envs\\ML\\lib\\site-packages\\mxnet\\gluon\\parameter.py:320: UserWarning: Parameter conv134_weight is already initialized, ignoring. Set force_reinit=True to re-initialize.\n",
      "  \"Set force_reinit=True to re-initialize.\"%self.name)\n",
      "C:\\Program Files\\Anaconda3\\envs\\ML\\lib\\site-packages\\mxnet\\gluon\\parameter.py:320: UserWarning: Parameter conv134_bias is already initialized, ignoring. Set force_reinit=True to re-initialize.\n",
      "  \"Set force_reinit=True to re-initialize.\"%self.name)\n",
      "C:\\Program Files\\Anaconda3\\envs\\ML\\lib\\site-packages\\mxnet\\gluon\\parameter.py:320: UserWarning: Parameter conv135_weight is already initialized, ignoring. Set force_reinit=True to re-initialize.\n",
      "  \"Set force_reinit=True to re-initialize.\"%self.name)\n",
      "C:\\Program Files\\Anaconda3\\envs\\ML\\lib\\site-packages\\mxnet\\gluon\\parameter.py:320: UserWarning: Parameter conv135_bias is already initialized, ignoring. Set force_reinit=True to re-initialize.\n",
      "  \"Set force_reinit=True to re-initialize.\"%self.name)\n",
      "C:\\Program Files\\Anaconda3\\envs\\ML\\lib\\site-packages\\mxnet\\gluon\\parameter.py:320: UserWarning: Parameter conv136_weight is already initialized, ignoring. Set force_reinit=True to re-initialize.\n",
      "  \"Set force_reinit=True to re-initialize.\"%self.name)\n",
      "C:\\Program Files\\Anaconda3\\envs\\ML\\lib\\site-packages\\mxnet\\gluon\\parameter.py:320: UserWarning: Parameter conv136_bias is already initialized, ignoring. Set force_reinit=True to re-initialize.\n",
      "  \"Set force_reinit=True to re-initialize.\"%self.name)\n",
      "C:\\Program Files\\Anaconda3\\envs\\ML\\lib\\site-packages\\mxnet\\gluon\\parameter.py:320: UserWarning: Parameter conv137_weight is already initialized, ignoring. Set force_reinit=True to re-initialize.\n",
      "  \"Set force_reinit=True to re-initialize.\"%self.name)\n",
      "C:\\Program Files\\Anaconda3\\envs\\ML\\lib\\site-packages\\mxnet\\gluon\\parameter.py:320: UserWarning: Parameter conv137_bias is already initialized, ignoring. Set force_reinit=True to re-initialize.\n",
      "  \"Set force_reinit=True to re-initialize.\"%self.name)\n",
      "C:\\Program Files\\Anaconda3\\envs\\ML\\lib\\site-packages\\mxnet\\gluon\\parameter.py:320: UserWarning: Parameter conv138_weight is already initialized, ignoring. Set force_reinit=True to re-initialize.\n",
      "  \"Set force_reinit=True to re-initialize.\"%self.name)\n",
      "C:\\Program Files\\Anaconda3\\envs\\ML\\lib\\site-packages\\mxnet\\gluon\\parameter.py:320: UserWarning: Parameter conv138_bias is already initialized, ignoring. Set force_reinit=True to re-initialize.\n",
      "  \"Set force_reinit=True to re-initialize.\"%self.name)\n",
      "C:\\Program Files\\Anaconda3\\envs\\ML\\lib\\site-packages\\mxnet\\gluon\\parameter.py:320: UserWarning: Parameter conv139_weight is already initialized, ignoring. Set force_reinit=True to re-initialize.\n",
      "  \"Set force_reinit=True to re-initialize.\"%self.name)\n",
      "C:\\Program Files\\Anaconda3\\envs\\ML\\lib\\site-packages\\mxnet\\gluon\\parameter.py:320: UserWarning: Parameter conv139_bias is already initialized, ignoring. Set force_reinit=True to re-initialize.\n",
      "  \"Set force_reinit=True to re-initialize.\"%self.name)\n",
      "C:\\Program Files\\Anaconda3\\envs\\ML\\lib\\site-packages\\mxnet\\gluon\\parameter.py:320: UserWarning: Parameter conv140_weight is already initialized, ignoring. Set force_reinit=True to re-initialize.\n",
      "  \"Set force_reinit=True to re-initialize.\"%self.name)\n",
      "C:\\Program Files\\Anaconda3\\envs\\ML\\lib\\site-packages\\mxnet\\gluon\\parameter.py:320: UserWarning: Parameter conv140_bias is already initialized, ignoring. Set force_reinit=True to re-initialize.\n",
      "  \"Set force_reinit=True to re-initialize.\"%self.name)\n",
      "C:\\Program Files\\Anaconda3\\envs\\ML\\lib\\site-packages\\mxnet\\gluon\\parameter.py:320: UserWarning: Parameter conv141_weight is already initialized, ignoring. Set force_reinit=True to re-initialize.\n",
      "  \"Set force_reinit=True to re-initialize.\"%self.name)\n",
      "C:\\Program Files\\Anaconda3\\envs\\ML\\lib\\site-packages\\mxnet\\gluon\\parameter.py:320: UserWarning: Parameter conv141_bias is already initialized, ignoring. Set force_reinit=True to re-initialize.\n",
      "  \"Set force_reinit=True to re-initialize.\"%self.name)\n",
      "C:\\Program Files\\Anaconda3\\envs\\ML\\lib\\site-packages\\mxnet\\gluon\\parameter.py:320: UserWarning: Parameter conv142_weight is already initialized, ignoring. Set force_reinit=True to re-initialize.\n",
      "  \"Set force_reinit=True to re-initialize.\"%self.name)\n",
      "C:\\Program Files\\Anaconda3\\envs\\ML\\lib\\site-packages\\mxnet\\gluon\\parameter.py:320: UserWarning: Parameter conv142_bias is already initialized, ignoring. Set force_reinit=True to re-initialize.\n",
      "  \"Set force_reinit=True to re-initialize.\"%self.name)\n",
      "C:\\Program Files\\Anaconda3\\envs\\ML\\lib\\site-packages\\mxnet\\gluon\\parameter.py:320: UserWarning: Parameter conv143_weight is already initialized, ignoring. Set force_reinit=True to re-initialize.\n",
      "  \"Set force_reinit=True to re-initialize.\"%self.name)\n",
      "C:\\Program Files\\Anaconda3\\envs\\ML\\lib\\site-packages\\mxnet\\gluon\\parameter.py:320: UserWarning: Parameter conv143_bias is already initialized, ignoring. Set force_reinit=True to re-initialize.\n",
      "  \"Set force_reinit=True to re-initialize.\"%self.name)\n",
      "C:\\Program Files\\Anaconda3\\envs\\ML\\lib\\site-packages\\mxnet\\gluon\\parameter.py:320: UserWarning: Parameter conv144_weight is already initialized, ignoring. Set force_reinit=True to re-initialize.\n",
      "  \"Set force_reinit=True to re-initialize.\"%self.name)\n",
      "C:\\Program Files\\Anaconda3\\envs\\ML\\lib\\site-packages\\mxnet\\gluon\\parameter.py:320: UserWarning: Parameter conv144_bias is already initialized, ignoring. Set force_reinit=True to re-initialize.\n",
      "  \"Set force_reinit=True to re-initialize.\"%self.name)\n",
      "C:\\Program Files\\Anaconda3\\envs\\ML\\lib\\site-packages\\mxnet\\gluon\\parameter.py:320: UserWarning: Parameter conv145_weight is already initialized, ignoring. Set force_reinit=True to re-initialize.\n",
      "  \"Set force_reinit=True to re-initialize.\"%self.name)\n",
      "C:\\Program Files\\Anaconda3\\envs\\ML\\lib\\site-packages\\mxnet\\gluon\\parameter.py:320: UserWarning: Parameter conv145_bias is already initialized, ignoring. Set force_reinit=True to re-initialize.\n",
      "  \"Set force_reinit=True to re-initialize.\"%self.name)\n",
      "C:\\Program Files\\Anaconda3\\envs\\ML\\lib\\site-packages\\mxnet\\gluon\\parameter.py:320: UserWarning: Parameter conv146_weight is already initialized, ignoring. Set force_reinit=True to re-initialize.\n",
      "  \"Set force_reinit=True to re-initialize.\"%self.name)\n",
      "C:\\Program Files\\Anaconda3\\envs\\ML\\lib\\site-packages\\mxnet\\gluon\\parameter.py:320: UserWarning: Parameter conv146_bias is already initialized, ignoring. Set force_reinit=True to re-initialize.\n",
      "  \"Set force_reinit=True to re-initialize.\"%self.name)\n",
      "C:\\Program Files\\Anaconda3\\envs\\ML\\lib\\site-packages\\mxnet\\gluon\\parameter.py:320: UserWarning: Parameter conv147_weight is already initialized, ignoring. Set force_reinit=True to re-initialize.\n",
      "  \"Set force_reinit=True to re-initialize.\"%self.name)\n",
      "C:\\Program Files\\Anaconda3\\envs\\ML\\lib\\site-packages\\mxnet\\gluon\\parameter.py:320: UserWarning: Parameter conv147_bias is already initialized, ignoring. Set force_reinit=True to re-initialize.\n",
      "  \"Set force_reinit=True to re-initialize.\"%self.name)\n",
      "C:\\Program Files\\Anaconda3\\envs\\ML\\lib\\site-packages\\mxnet\\gluon\\parameter.py:320: UserWarning: Parameter conv148_weight is already initialized, ignoring. Set force_reinit=True to re-initialize.\n",
      "  \"Set force_reinit=True to re-initialize.\"%self.name)\n",
      "C:\\Program Files\\Anaconda3\\envs\\ML\\lib\\site-packages\\mxnet\\gluon\\parameter.py:320: UserWarning: Parameter conv148_bias is already initialized, ignoring. Set force_reinit=True to re-initialize.\n",
      "  \"Set force_reinit=True to re-initialize.\"%self.name)\n",
      "C:\\Program Files\\Anaconda3\\envs\\ML\\lib\\site-packages\\mxnet\\gluon\\parameter.py:320: UserWarning: Parameter conv149_weight is already initialized, ignoring. Set force_reinit=True to re-initialize.\n",
      "  \"Set force_reinit=True to re-initialize.\"%self.name)\n",
      "C:\\Program Files\\Anaconda3\\envs\\ML\\lib\\site-packages\\mxnet\\gluon\\parameter.py:320: UserWarning: Parameter conv149_bias is already initialized, ignoring. Set force_reinit=True to re-initialize.\n",
      "  \"Set force_reinit=True to re-initialize.\"%self.name)\n",
      "C:\\Program Files\\Anaconda3\\envs\\ML\\lib\\site-packages\\mxnet\\gluon\\parameter.py:320: UserWarning: Parameter conv150_weight is already initialized, ignoring. Set force_reinit=True to re-initialize.\n",
      "  \"Set force_reinit=True to re-initialize.\"%self.name)\n",
      "C:\\Program Files\\Anaconda3\\envs\\ML\\lib\\site-packages\\mxnet\\gluon\\parameter.py:320: UserWarning: Parameter conv150_bias is already initialized, ignoring. Set force_reinit=True to re-initialize.\n",
      "  \"Set force_reinit=True to re-initialize.\"%self.name)\n",
      "C:\\Program Files\\Anaconda3\\envs\\ML\\lib\\site-packages\\mxnet\\gluon\\parameter.py:320: UserWarning: Parameter conv151_weight is already initialized, ignoring. Set force_reinit=True to re-initialize.\n",
      "  \"Set force_reinit=True to re-initialize.\"%self.name)\n",
      "C:\\Program Files\\Anaconda3\\envs\\ML\\lib\\site-packages\\mxnet\\gluon\\parameter.py:320: UserWarning: Parameter conv151_bias is already initialized, ignoring. Set force_reinit=True to re-initialize.\n",
      "  \"Set force_reinit=True to re-initialize.\"%self.name)\n",
      "C:\\Program Files\\Anaconda3\\envs\\ML\\lib\\site-packages\\mxnet\\gluon\\parameter.py:320: UserWarning: Parameter conv152_weight is already initialized, ignoring. Set force_reinit=True to re-initialize.\n",
      "  \"Set force_reinit=True to re-initialize.\"%self.name)\n",
      "C:\\Program Files\\Anaconda3\\envs\\ML\\lib\\site-packages\\mxnet\\gluon\\parameter.py:320: UserWarning: Parameter conv152_bias is already initialized, ignoring. Set force_reinit=True to re-initialize.\n",
      "  \"Set force_reinit=True to re-initialize.\"%self.name)\n",
      "C:\\Program Files\\Anaconda3\\envs\\ML\\lib\\site-packages\\mxnet\\gluon\\parameter.py:320: UserWarning: Parameter conv153_weight is already initialized, ignoring. Set force_reinit=True to re-initialize.\n",
      "  \"Set force_reinit=True to re-initialize.\"%self.name)\n",
      "C:\\Program Files\\Anaconda3\\envs\\ML\\lib\\site-packages\\mxnet\\gluon\\parameter.py:320: UserWarning: Parameter conv153_bias is already initialized, ignoring. Set force_reinit=True to re-initialize.\n",
      "  \"Set force_reinit=True to re-initialize.\"%self.name)\n",
      "C:\\Program Files\\Anaconda3\\envs\\ML\\lib\\site-packages\\mxnet\\gluon\\parameter.py:320: UserWarning: Parameter conv154_weight is already initialized, ignoring. Set force_reinit=True to re-initialize.\n",
      "  \"Set force_reinit=True to re-initialize.\"%self.name)\n",
      "C:\\Program Files\\Anaconda3\\envs\\ML\\lib\\site-packages\\mxnet\\gluon\\parameter.py:320: UserWarning: Parameter conv154_bias is already initialized, ignoring. Set force_reinit=True to re-initialize.\n",
      "  \"Set force_reinit=True to re-initialize.\"%self.name)\n",
      "C:\\Program Files\\Anaconda3\\envs\\ML\\lib\\site-packages\\mxnet\\gluon\\parameter.py:320: UserWarning: Parameter conv155_weight is already initialized, ignoring. Set force_reinit=True to re-initialize.\n",
      "  \"Set force_reinit=True to re-initialize.\"%self.name)\n",
      "C:\\Program Files\\Anaconda3\\envs\\ML\\lib\\site-packages\\mxnet\\gluon\\parameter.py:320: UserWarning: Parameter conv155_bias is already initialized, ignoring. Set force_reinit=True to re-initialize.\n",
      "  \"Set force_reinit=True to re-initialize.\"%self.name)\n",
      "C:\\Program Files\\Anaconda3\\envs\\ML\\lib\\site-packages\\mxnet\\gluon\\parameter.py:320: UserWarning: Parameter conv156_weight is already initialized, ignoring. Set force_reinit=True to re-initialize.\n",
      "  \"Set force_reinit=True to re-initialize.\"%self.name)\n",
      "C:\\Program Files\\Anaconda3\\envs\\ML\\lib\\site-packages\\mxnet\\gluon\\parameter.py:320: UserWarning: Parameter conv156_bias is already initialized, ignoring. Set force_reinit=True to re-initialize.\n",
      "  \"Set force_reinit=True to re-initialize.\"%self.name)\n",
      "C:\\Program Files\\Anaconda3\\envs\\ML\\lib\\site-packages\\mxnet\\gluon\\parameter.py:320: UserWarning: Parameter conv157_weight is already initialized, ignoring. Set force_reinit=True to re-initialize.\n",
      "  \"Set force_reinit=True to re-initialize.\"%self.name)\n",
      "C:\\Program Files\\Anaconda3\\envs\\ML\\lib\\site-packages\\mxnet\\gluon\\parameter.py:320: UserWarning: Parameter conv157_bias is already initialized, ignoring. Set force_reinit=True to re-initialize.\n",
      "  \"Set force_reinit=True to re-initialize.\"%self.name)\n",
      "C:\\Program Files\\Anaconda3\\envs\\ML\\lib\\site-packages\\mxnet\\gluon\\parameter.py:320: UserWarning: Parameter conv158_weight is already initialized, ignoring. Set force_reinit=True to re-initialize.\n",
      "  \"Set force_reinit=True to re-initialize.\"%self.name)\n",
      "C:\\Program Files\\Anaconda3\\envs\\ML\\lib\\site-packages\\mxnet\\gluon\\parameter.py:320: UserWarning: Parameter conv158_bias is already initialized, ignoring. Set force_reinit=True to re-initialize.\n",
      "  \"Set force_reinit=True to re-initialize.\"%self.name)\n",
      "C:\\Program Files\\Anaconda3\\envs\\ML\\lib\\site-packages\\mxnet\\gluon\\data\\vision.py:118: DeprecationWarning: The binary mode of fromstring is deprecated, as it behaves surprisingly on unicode inputs. Use frombuffer instead\n",
      "  label = np.fromstring(fin.read(), dtype=np.uint8).astype(np.int32)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Program Files\\Anaconda3\\envs\\ML\\lib\\site-packages\\mxnet\\gluon\\data\\vision.py:122: DeprecationWarning: The binary mode of fromstring is deprecated, as it behaves surprisingly on unicode inputs. Use frombuffer instead\n",
      "  data = np.fromstring(fin.read(), dtype=np.uint8)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training on  gpu(0)\n",
      "epoch 1, loss 0.5049, train acc 0.819, test acc 0.865, time 59.2 sec\n",
      "epoch 2, loss 0.3455, train acc 0.869, test acc 0.873, time 59.2 sec\n",
      "epoch 3, loss 0.3119, train acc 0.881, test acc 0.874, time 59.4 sec\n",
      "epoch 4, loss 0.2891, train acc 0.890, test acc 0.878, time 59.5 sec\n",
      "epoch 5, loss nan, train acc 0.477, test acc 0.100, time 58.8 sec\n",
      "epoch 6, loss nan, train acc 0.100, test acc 0.100, time 58.7 sec\n",
      "epoch 7, loss nan, train acc 0.100, test acc 0.100, time 58.7 sec\n",
      "epoch 8, loss nan, train acc 0.100, test acc 0.100, time 58.8 sec\n",
      "epoch 9, loss nan, train acc 0.100, test acc 0.100, time 58.6 sec\n",
      "epoch 10, loss nan, train acc 0.100, test acc 0.100, time 58.6 sec\n",
      "epoch 11, loss nan, train acc 0.100, test acc 0.100, time 58.8 sec\n",
      "epoch 12, loss nan, train acc 0.100, test acc 0.100, time 58.8 sec\n",
      "epoch 13, loss nan, train acc 0.100, test acc 0.100, time 58.7 sec\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-28-1f23650585b7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgb\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_data_fashion_mnist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m128\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m96\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgluon\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSoftmaxCrossEntropyLoss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[0mgb\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnet\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mctx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m150\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Downloads\\gluon-tutorials-zh\\gluonbook\\utils.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(train_iter, test_iter, net, loss, trainer, ctx, num_epochs, print_batches)\u001b[0m\n\u001b[0;32m    182\u001b[0m                 \u001b[0ml\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    183\u001b[0m             train_acc_sum += sum([(y_hat.argmax(axis=1) == y).sum().asscalar()\n\u001b[1;32m--> 184\u001b[1;33m                                  for y_hat, y in zip(y_hats, ys)])\n\u001b[0m\u001b[0;32m    185\u001b[0m             \u001b[0mtrain_l_sum\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0ml\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masscalar\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0ml\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mls\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    186\u001b[0m             \u001b[0mtrainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Downloads\\gluon-tutorials-zh\\gluonbook\\utils.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    182\u001b[0m                 \u001b[0ml\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    183\u001b[0m             train_acc_sum += sum([(y_hat.argmax(axis=1) == y).sum().asscalar()\n\u001b[1;32m--> 184\u001b[1;33m                                  for y_hat, y in zip(y_hats, ys)])\n\u001b[0m\u001b[0;32m    185\u001b[0m             \u001b[0mtrain_l_sum\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0ml\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masscalar\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0ml\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mls\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    186\u001b[0m             \u001b[0mtrainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Program Files\\Anaconda3\\envs\\ML\\lib\\site-packages\\mxnet\\ndarray\\ndarray.py\u001b[0m in \u001b[0;36masscalar\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1809\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1810\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"The current array is not a scalar\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1811\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1812\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1813\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Program Files\\Anaconda3\\envs\\ML\\lib\\site-packages\\mxnet\\ndarray\\ndarray.py\u001b[0m in \u001b[0;36masnumpy\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1791\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1792\u001b[0m             \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mctypes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata_as\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mctypes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mc_void_p\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1793\u001b[1;33m             ctypes.c_size_t(data.size)))\n\u001b[0m\u001b[0;32m   1794\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1795\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "lr = 0.1\n",
    "ctx = gb.try_gpu()\n",
    "net.initialize( ctx=ctx, init=init.Xavier())\n",
    "trainer = gluon.Trainer(net.collect_params(), 'sgd', {'learning_rate': lr})\n",
    "train_data, test_data = gb.load_data_fashion_mnist(batch_size=128, resize=96)\n",
    "loss = gluon.loss.SoftmaxCrossEntropyLoss()\n",
    "gb.train(train_data, test_data, net, loss, trainer, ctx, num_epochs=150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Program Files\\Anaconda3\\envs\\ML\\lib\\site-packages\\mxnet\\gluon\\data\\vision.py:118: DeprecationWarning: The binary mode of fromstring is deprecated, as it behaves surprisingly on unicode inputs. Use frombuffer instead\n",
      "  label = np.fromstring(fin.read(), dtype=np.uint8).astype(np.int32)\n",
      "C:\\Program Files\\Anaconda3\\envs\\ML\\lib\\site-packages\\mxnet\\gluon\\data\\vision.py:122: DeprecationWarning: The binary mode of fromstring is deprecated, as it behaves surprisingly on unicode inputs. Use frombuffer instead\n",
      "  data = np.fromstring(fin.read(), dtype=np.uint8)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training on  gpu(0)\n",
      "epoch 1, loss 1.9175, train acc 0.327, test acc 0.674, time 65.0 sec\n",
      "epoch 2, loss 0.6988, train acc 0.737, test acc 0.756, time 59.5 sec\n",
      "epoch 3, loss 0.4983, train acc 0.812, test acc 0.845, time 59.7 sec\n",
      "epoch 4, loss 0.4189, train acc 0.842, test acc 0.857, time 59.8 sec\n",
      "epoch 5, loss 0.3732, train acc 0.858, test acc 0.867, time 59.8 sec\n",
      "epoch 6, loss 0.3453, train acc 0.869, test acc 0.881, time 59.9 sec\n",
      "epoch 7, loss 0.3228, train acc 0.877, test acc 0.878, time 60.0 sec\n",
      "epoch 8, loss 0.3034, train acc 0.885, test acc 0.881, time 60.0 sec\n",
      "epoch 9, loss 0.2886, train acc 0.890, test acc 0.893, time 59.8 sec\n",
      "epoch 10, loss 0.2745, train acc 0.895, test acc 0.894, time 59.9 sec\n",
      "epoch 11, loss 0.2630, train acc 0.900, test acc 0.890, time 60.0 sec\n",
      "epoch 12, loss 0.2503, train acc 0.905, test acc 0.895, time 59.9 sec\n",
      "epoch 13, loss 0.2415, train acc 0.908, test acc 0.898, time 59.9 sec\n",
      "epoch 14, loss 0.2310, train acc 0.913, test acc 0.903, time 59.9 sec\n",
      "epoch 15, loss 0.2212, train acc 0.915, test acc 0.905, time 59.9 sec\n",
      "epoch 16, loss 0.2110, train acc 0.920, test acc 0.887, time 59.9 sec\n",
      "epoch 17, loss 0.2037, train acc 0.922, test acc 0.897, time 59.9 sec\n",
      "epoch 18, loss 0.3862, train acc 0.866, test acc 0.506, time 59.9 sec\n",
      "epoch 19, loss 0.3974, train acc 0.854, test acc 0.902, time 59.8 sec\n",
      "epoch 20, loss 0.2139, train acc 0.919, test acc 0.904, time 60.0 sec\n",
      "epoch 21, loss 0.1925, train acc 0.927, test acc 0.905, time 59.9 sec\n",
      "epoch 22, loss 0.1833, train acc 0.929, test acc 0.906, time 59.9 sec\n",
      "epoch 23, loss 0.1685, train acc 0.935, test acc 0.912, time 60.0 sec\n",
      "epoch 24, loss 0.1598, train acc 0.940, test acc 0.913, time 59.9 sec\n",
      "epoch 25, loss 0.1559, train acc 0.940, test acc 0.915, time 59.9 sec\n",
      "epoch 26, loss 0.1453, train acc 0.945, test acc 0.911, time 59.9 sec\n",
      "epoch 27, loss 0.3036, train acc 0.895, test acc 0.908, time 59.9 sec\n",
      "epoch 28, loss 0.1617, train acc 0.939, test acc 0.906, time 59.9 sec\n",
      "epoch 29, loss 0.1347, train acc 0.949, test acc 0.918, time 59.8 sec\n",
      "epoch 30, loss 0.1260, train acc 0.952, test acc 0.921, time 59.9 sec\n",
      "epoch 31, loss 0.1181, train acc 0.954, test acc 0.912, time 59.9 sec\n",
      "epoch 32, loss 0.1421, train acc 0.948, test acc 0.921, time 59.8 sec\n",
      "epoch 33, loss 0.1038, train acc 0.960, test acc 0.909, time 59.9 sec\n",
      "epoch 34, loss 0.0967, train acc 0.963, test acc 0.911, time 59.9 sec\n",
      "epoch 35, loss 0.0966, train acc 0.963, test acc 0.912, time 59.8 sec\n",
      "epoch 36, loss 0.0894, train acc 0.965, test acc 0.917, time 59.9 sec\n",
      "epoch 37, loss 0.0827, train acc 0.969, test acc 0.920, time 60.0 sec\n",
      "epoch 38, loss 0.0767, train acc 0.971, test acc 0.915, time 59.8 sec\n",
      "epoch 39, loss 0.0750, train acc 0.971, test acc 0.917, time 60.0 sec\n",
      "epoch 40, loss 0.0762, train acc 0.972, test acc 0.908, time 59.8 sec\n",
      "epoch 41, loss 0.0652, train acc 0.975, test acc 0.910, time 59.8 sec\n",
      "epoch 42, loss 0.0700, train acc 0.973, test acc 0.916, time 59.7 sec\n",
      "epoch 43, loss 0.0682, train acc 0.975, test acc 0.919, time 59.9 sec\n",
      "epoch 44, loss 0.0562, train acc 0.979, test acc 0.915, time 59.8 sec\n",
      "epoch 45, loss 0.0595, train acc 0.978, test acc 0.921, time 59.9 sec\n",
      "epoch 46, loss 0.0468, train acc 0.983, test acc 0.917, time 59.9 sec\n",
      "epoch 47, loss 0.0499, train acc 0.981, test acc 0.919, time 59.8 sec\n",
      "epoch 48, loss 0.0545, train acc 0.979, test acc 0.914, time 59.8 sec\n",
      "epoch 49, loss 0.0408, train acc 0.984, test acc 0.911, time 59.7 sec\n",
      "epoch 50, loss 0.0442, train acc 0.983, test acc 0.920, time 59.8 sec\n",
      "epoch 51, loss 0.0469, train acc 0.983, test acc 0.914, time 59.9 sec\n",
      "epoch 52, loss 0.0334, train acc 0.987, test acc 0.916, time 59.9 sec\n",
      "epoch 53, loss 0.0383, train acc 0.986, test acc 0.921, time 59.8 sec\n",
      "epoch 54, loss 0.0370, train acc 0.987, test acc 0.915, time 59.9 sec\n",
      "epoch 55, loss 0.0329, train acc 0.989, test acc 0.920, time 59.7 sec\n",
      "epoch 56, loss 0.0325, train acc 0.988, test acc 0.917, time 59.7 sec\n",
      "epoch 57, loss 0.0264, train acc 0.991, test acc 0.923, time 59.8 sec\n",
      "epoch 58, loss 0.0315, train acc 0.989, test acc 0.919, time 60.0 sec\n",
      "epoch 59, loss 0.0264, train acc 0.991, test acc 0.921, time 59.9 sec\n",
      "epoch 60, loss 0.0304, train acc 0.990, test acc 0.919, time 59.8 sec\n",
      "epoch 61, loss 0.0226, train acc 0.993, test acc 0.921, time 59.8 sec\n",
      "epoch 62, loss 0.0347, train acc 0.989, test acc 0.918, time 59.9 sec\n",
      "epoch 63, loss 0.0178, train acc 0.994, test acc 0.916, time 59.9 sec\n",
      "epoch 64, loss 0.0189, train acc 0.994, test acc 0.919, time 60.1 sec\n",
      "epoch 65, loss 0.0235, train acc 0.992, test acc 0.924, time 59.9 sec\n",
      "epoch 66, loss 0.0163, train acc 0.995, test acc 0.919, time 60.0 sec\n",
      "epoch 67, loss 0.0168, train acc 0.995, test acc 0.915, time 59.9 sec\n",
      "epoch 68, loss 0.0170, train acc 0.995, test acc 0.922, time 59.9 sec\n",
      "epoch 69, loss 0.0161, train acc 0.995, test acc 0.894, time 59.8 sec\n",
      "epoch 70, loss 0.0239, train acc 0.992, test acc 0.919, time 60.0 sec\n",
      "epoch 71, loss 0.0179, train acc 0.994, test acc 0.917, time 59.8 sec\n",
      "epoch 72, loss 0.0152, train acc 0.995, test acc 0.918, time 59.9 sec\n",
      "epoch 73, loss 0.0197, train acc 0.993, test acc 0.916, time 59.9 sec\n",
      "epoch 74, loss 0.0116, train acc 0.996, test acc 0.919, time 59.8 sec\n",
      "epoch 75, loss 0.0049, train acc 0.999, test acc 0.919, time 59.9 sec\n",
      "epoch 76, loss 0.0178, train acc 0.994, test acc 0.914, time 60.0 sec\n",
      "epoch 77, loss 0.0049, train acc 0.999, test acc 0.921, time 59.9 sec\n",
      "epoch 78, loss 0.0138, train acc 0.996, test acc 0.920, time 60.0 sec\n",
      "epoch 79, loss 0.0123, train acc 0.996, test acc 0.919, time 59.9 sec\n",
      "epoch 80, loss 0.0103, train acc 0.997, test acc 0.921, time 60.2 sec\n",
      "epoch 81, loss 0.0053, train acc 0.998, test acc 0.919, time 59.9 sec\n",
      "epoch 82, loss 0.0121, train acc 0.996, test acc 0.916, time 59.9 sec\n",
      "epoch 83, loss 0.0138, train acc 0.996, test acc 0.920, time 59.8 sec\n",
      "epoch 84, loss 0.0108, train acc 0.997, test acc 0.922, time 59.9 sec\n",
      "epoch 85, loss 0.0016, train acc 1.000, test acc 0.922, time 59.9 sec\n",
      "epoch 86, loss 0.0002, train acc 1.000, test acc 0.924, time 59.9 sec\n",
      "epoch 87, loss 0.0000, train acc 1.000, test acc 0.924, time 59.9 sec\n",
      "epoch 88, loss 0.0000, train acc 1.000, test acc 0.924, time 59.8 sec\n",
      "epoch 89, loss 0.0000, train acc 1.000, test acc 0.924, time 59.7 sec\n",
      "epoch 90, loss 0.0000, train acc 1.000, test acc 0.924, time 59.7 sec\n",
      "epoch 91, loss 0.0000, train acc 1.000, test acc 0.925, time 59.9 sec\n",
      "epoch 92, loss 0.0000, train acc 1.000, test acc 0.925, time 59.9 sec\n",
      "epoch 93, loss 0.0000, train acc 1.000, test acc 0.925, time 59.9 sec\n",
      "epoch 94, loss 0.0000, train acc 1.000, test acc 0.925, time 59.9 sec\n",
      "epoch 95, loss 0.0000, train acc 1.000, test acc 0.925, time 60.0 sec\n",
      "epoch 96, loss 0.0000, train acc 1.000, test acc 0.925, time 59.9 sec\n",
      "epoch 97, loss 0.0000, train acc 1.000, test acc 0.925, time 60.1 sec\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-2a48e5a6b435>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgb\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_data_fashion_mnist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m128\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m96\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgluon\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSoftmaxCrossEntropyLoss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[0mgb\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnet\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mctx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m150\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Downloads\\gluon-tutorials-zh\\gluonbook\\utils.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(train_iter, test_iter, net, loss, trainer, ctx, num_epochs, print_batches)\u001b[0m\n\u001b[0;32m    182\u001b[0m                 \u001b[0ml\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    183\u001b[0m             train_acc_sum += sum([(y_hat.argmax(axis=1) == y).sum().asscalar()\n\u001b[1;32m--> 184\u001b[1;33m                                  for y_hat, y in zip(y_hats, ys)])\n\u001b[0m\u001b[0;32m    185\u001b[0m             \u001b[0mtrain_l_sum\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0ml\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masscalar\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0ml\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mls\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    186\u001b[0m             \u001b[0mtrainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Downloads\\gluon-tutorials-zh\\gluonbook\\utils.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    182\u001b[0m                 \u001b[0ml\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    183\u001b[0m             train_acc_sum += sum([(y_hat.argmax(axis=1) == y).sum().asscalar()\n\u001b[1;32m--> 184\u001b[1;33m                                  for y_hat, y in zip(y_hats, ys)])\n\u001b[0m\u001b[0;32m    185\u001b[0m             \u001b[0mtrain_l_sum\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0ml\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masscalar\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0ml\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mls\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    186\u001b[0m             \u001b[0mtrainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Program Files\\Anaconda3\\envs\\ML\\lib\\site-packages\\mxnet\\ndarray\\ndarray.py\u001b[0m in \u001b[0;36masscalar\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1809\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1810\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"The current array is not a scalar\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1811\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1812\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1813\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Program Files\\Anaconda3\\envs\\ML\\lib\\site-packages\\mxnet\\ndarray\\ndarray.py\u001b[0m in \u001b[0;36masnumpy\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1791\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1792\u001b[0m             \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mctypes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata_as\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mctypes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mc_void_p\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1793\u001b[1;33m             ctypes.c_size_t(data.size)))\n\u001b[0m\u001b[0;32m   1794\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1795\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "lr = 0.05\n",
    "ctx = gb.try_gpu()\n",
    "net.initialize( ctx=ctx, init=init.Xavier())\n",
    "trainer = gluon.Trainer(net.collect_params(), 'sgd', {'learning_rate': lr})\n",
    "train_data, test_data = gb.load_data_fashion_mnist(batch_size=128, resize=96)\n",
    "loss = gluon.loss.SoftmaxCrossEntropyLoss()\n",
    "gb.train(train_data, test_data, net, loss, trainer, ctx, num_epochs=150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
